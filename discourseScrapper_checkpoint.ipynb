{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nguepigit2020/StemAway_Recommender/blob/main/discourseScrapper_checkpoint.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ca4345f",
      "metadata": {
        "id": "1ca4345f"
      },
      "source": [
        "# implement\n",
        "\n",
        "1. Non-Repeating Links [Done]\n",
        "\n",
        "2. Save Form Stats [Done]\n",
        "\n",
        "3. Proper scroll to the bottom [Done]\n",
        "\n",
        "4. Filter common phrases [Done]\n",
        "\n",
        "5. Regex (A-Z,(,),-) [Done]\n",
        "\n",
        "6. Differentiate between english and other languages [Working On]\n",
        "\n",
        "7. Look over Data (Are the words being retreived okay?)[Working On (Almost done)]\n",
        "\n",
        "8. Make code efficient (Any way you can make it so it runs a bit faster? Ex. Figuring out where timeouts are needed and where not)\n",
        "\n",
        "9. Make code cleaner (comments, proper PEP, convert to pyfile, FIX UR GODAMN VARIABLE NAMES)\n",
        "\n",
        "10. Remove any functions which you dont need into \"extraFunc.py\"\n",
        "\n",
        "11. Upload to github\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "5c2cb476",
      "metadata": {
        "id": "5c2cb476"
      },
      "outputs": [],
      "source": [
        "class discourseWordScraper:\n",
        "    \n",
        "    def __init__ (self, basicLink, websitePage, specificClasses = [], specificTags = [], specificIds = [], specificCus = [], dicCusClass = []):\n",
        "        \n",
        "    \n",
        "        self.basicLink = basicLink\n",
        "        self.websitePage = websitePage\n",
        "        \n",
        "        self.specificClasses = specificClasses\n",
        "        self.specificTags = specificTags\n",
        "        self.specificIds = specificIds\n",
        "\n",
        "        self.bSoup = None\n",
        "\n",
        "        \n",
        "    def setVar(self, htmlData, websitePage):\n",
        "        self.bSoup = BeautifulSoup(htmlData.strip(), 'html.parser')\n",
        "        self.websitePage = websitePage\n",
        "        \n",
        "    def getByClass(self):\n",
        "        \n",
        "        tempData = list()\n",
        "        \n",
        "        for classTxt in self.specificClasses:\n",
        "            tempData += self.bSoup.find_all(class_=classTxt)\n",
        "            \n",
        "            \n",
        "        return ([x.text.split() for x in tempData], self.specificClasses)\n",
        "    \n",
        "    def getById(self):\n",
        "        \n",
        "        tempData = list()\n",
        "        \n",
        "        for idTxt in self.specificIds:\n",
        "            tempData += self.bSoup.find_all(id=idTxt)\n",
        "        return ([x.text.split('[,.\\s]') for x in tempData], self.specificId)\n",
        "        \n",
        "            \n",
        "    def getByTag(self):\n",
        "        \n",
        "        tempData = list()\n",
        "        \n",
        "        for tagTxt in self.specificTags:\n",
        "            tempData += self.bSoup.find_all(tagTxt)\n",
        "            \n",
        "        \n",
        "        return([x.text.split('[,.\\s]') for x in tempData], self.specificTags)\n",
        "        \n",
        "\n",
        "    \n",
        "    def getAllRelText(self):\n",
        "       \n",
        "        for script in self.bSoup([\"script\", \"style\"]):\n",
        "            script.extract()\n",
        "        \n",
        "        text = \"ALLTXT \"+ self.bSoup.get_text()\n",
        "        print(text)\n",
        "        return ([text.replace('\\n', \" \").split('[,.\\s]')], \"AllTxt\")\n",
        "        \n",
        "         \n",
        "        \n",
        "    def validateInput(self, tempData, typeCases, statsDict, repeating = True):\n",
        "        \n",
        "        blackList = [\"http\",\"img\",\"<p>\"] #<- NEEDS INPUT TO RUN\n",
        "        regex = re.compile(\"[^a-zA-Z-/_']\")\n",
        "        finalInput = list()\n",
        "        \n",
        "        \n",
        "        for rawSentence in tempData:\n",
        "                rawInput = ([regex.sub('', string.lower()) for string in rawSentence])\n",
        "                \n",
        "                finalInput.extend([string for string in rawInput if any(blackTag in string for blackTag in blackList) == False and string != \"\"])                \n",
        "\n",
        "        \n",
        "        print(finalInput)\n",
        "        \n",
        "        print(statsDict[\"date-scraped\"])\n",
        "        finalInput.insert(0, statsDict[\"date-scraped\"])\n",
        "        finalInput.insert(1, (self.basicLink + self.websitePage + \"/\"))\n",
        "        finalInput.insert(2, statsDict[\"created\"])\n",
        "        finalInput.insert(3, statsDict[\"last reply\"])\n",
        "        \n",
        "        try: finalInput.insert(4, statsDict[\"replies\"]) \n",
        "        except: finalInput.insert(4, statsDict[\"reply\"])\n",
        "        \n",
        "        try: finalInput.insert(5, statsDict[\"views\"])\n",
        "        except: finalInput.insert(5, statsDict[\"view\"])\n",
        "        \n",
        "        try: finalInput.insert(6, statsDict[\"users\"]) \n",
        "        except: finalInput.insert(6, statsDict[\"user\"])\n",
        "        \n",
        "        try:finalInput.insert(7, statsDict[\"likes\"])\n",
        "        except: finalInput.insert(7, statsDict[\"like\"])\n",
        "        \n",
        "        \n",
        "        try: finalInput.insert(8, statsDict[\"links\"])\n",
        "        except: finalInput.insert(8, statsDict[\"link\"])\n",
        "        \n",
        "        finalInput.insert(9, typeCases)\n",
        "    \n",
        "\n",
        "            \n",
        "            \n",
        "        \n",
        "        return finalInput\n",
        "        \n",
        "            \n",
        "    def saveCSV(self, finalInput):\n",
        "        \n",
        "        with open('data.csv', 'r', encoding=\"utf-8\") as dfr, open('data.csv', 'a+', newline='', encoding=\"utf-8\") as dfw:\n",
        "            \n",
        "            \n",
        "            writer = csv.writer(dfw)\n",
        "            reader = csv.reader(dfr)\n",
        "           \n",
        "            writer.writerow(finalInput) \n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install selenium"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wva3MMQhEUjQ",
        "outputId": "f21e581d-f06b-4180-bb05-74e5d54c04f7"
      },
      "id": "Wva3MMQhEUjQ",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: selenium in /usr/local/lib/python3.7/dist-packages (4.3.0)\n",
            "Requirement already satisfied: urllib3[secure,socks]~=1.26 in /usr/local/lib/python3.7/dist-packages (from selenium) (1.26.11)\n",
            "Requirement already satisfied: trio~=0.17 in /usr/local/lib/python3.7/dist-packages (from selenium) (0.21.0)\n",
            "Requirement already satisfied: trio-websocket~=0.9 in /usr/local/lib/python3.7/dist-packages (from selenium) (0.9.2)\n",
            "Requirement already satisfied: outcome in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (1.2.0)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (21.4.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (1.2.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (2.10)\n",
            "Requirement already satisfied: async-generator>=1.9 in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (1.10)\n",
            "Requirement already satisfied: wsproto>=0.14 in /usr/local/lib/python3.7/dist-packages (from trio-websocket~=0.9->selenium) (1.1.0)\n",
            "Requirement already satisfied: cryptography>=1.3.4 in /usr/local/lib/python3.7/dist-packages (from urllib3[secure,socks]~=1.26->selenium) (37.0.4)\n",
            "Requirement already satisfied: pyOpenSSL>=0.14 in /usr/local/lib/python3.7/dist-packages (from urllib3[secure,socks]~=1.26->selenium) (22.0.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from urllib3[secure,socks]~=1.26->selenium) (2022.6.15)\n",
            "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from urllib3[secure,socks]~=1.26->selenium) (1.7.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=1.3.4->urllib3[secure,socks]~=1.26->selenium) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=1.3.4->urllib3[secure,socks]~=1.26->selenium) (2.21)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.13.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from h11<1,>=0.9.0->wsproto>=0.14->trio-websocket~=0.9->selenium) (4.1.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "95439adb",
      "metadata": {
        "id": "95439adb"
      },
      "outputs": [],
      "source": [
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import re\n",
        "import random\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "\n",
        "class formNavigator():\n",
        "    \n",
        "    def __init__ (self, basicLink, websitePage=\"\", blackList = {}):\n",
        "        \n",
        "        self.basicLink = basicLink\n",
        "        self.websitePage = websitePage\n",
        "        self.currentLink = basicLink + websitePage\n",
        "        \n",
        "        self.linksStored = list()\n",
        "        self.blackList = {set}\n",
        "        \n",
        "        self.bSoup = None\n",
        "        \n",
        "        selService = Service('/Users/idrissnguepi/Downloads/chromedriver')\n",
        "        \n",
        "        option = webdriver.ChromeOptions()\n",
        "        option.add_argument('--incognito')\n",
        "#         chrome_options.add_argument(\"--headless\")\n",
        "        \n",
        "        self.driver = webdriver.Chrome(service=selService, options=option)\n",
        "        \n",
        "    \n",
        "    \n",
        "    def scrollPage(self, userRequest = \"down\",numOfTimes=\"infinite\"):\n",
        "        \n",
        "        last_height = self.driver.execute_script(\"return document.body.scrollHeight\")\n",
        "        \n",
        "        \n",
        "        if numOfTimes == \"infinite\":\n",
        "        \n",
        "            while True:\n",
        "\n",
        "                if userRequest == \"up\": new_height = self.driver.execute_script(\"window.scrollTo(0,0)\")\n",
        "                elif userRequest == \"down\": new_height = self.driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
        "\n",
        "                time.sleep(2)\n",
        "\n",
        "                if new_height == last_height:\n",
        "                    break\n",
        "\n",
        "                last_height = new_height\n",
        "            \n",
        "        else:\n",
        "                \n",
        "            for num in range(int(numOfTimes)):\n",
        "                \n",
        "                self.driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
        "                print(\"hello\")\n",
        "                time.sleep(2)\n",
        "                \n",
        "\n",
        "\n",
        "                \n",
        "    \n",
        "    \n",
        "    def getPageHtml(self):\n",
        "        \n",
        "        self.driver.get(f'{self.currentLink}')\n",
        "        time.sleep(2)\n",
        "        \n",
        "        pageMeta = \"\"\n",
        "        \n",
        "        self.scrollPage(userRequest=\"up\")\n",
        "        \n",
        "        if \"/t/\" in self.currentLink:\n",
        "            pageMeta = self.getPageMeta(self.driver.page_source)\n",
        "            self.scrollPage(userRequest=\"down\")            \n",
        "            \n",
        "        elif \"/c/\" in self.currentLink: \n",
        "            self.scrollPage(numOfTimes = \"5\")\n",
        "            \n",
        "        else:\n",
        "            self.scrollPage(userRequest=\"down\")\n",
        "                \n",
        "        \n",
        "        with open(\"htmlStored.txt\", \"r+\", encoding='utf-8') as htmlPage:\n",
        "            htmlPage.write(self.driver.page_source) \n",
        "            \n",
        "        \n",
        "        self.bSoup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
        "        return self.bSoup, self.basicLink, self.websitePage, pageMeta\n",
        "    \n",
        "    def getPageMeta(self,html):\n",
        "        \n",
        "        print(self.currentLink)\n",
        "\n",
        "        now = datetime.now()\n",
        "        self.bSoup = BeautifulSoup(html, 'html.parser')\n",
        "        \n",
        "        try:\n",
        "            infoUList = self.bSoup.find(\"div\", class_=\"topic-map\")\n",
        "            \n",
        "            dateList = [date.get(\"title\") for date in infoUList.find_all(\"span\", class_=\"relative-date\") if date.get(\"title\") != None][0:2]\n",
        "            time.sleep(0.5)\n",
        "        \n",
        "            \n",
        "            liNumbers = [num.text for num in infoUList.find_all(\"span\", class_=\"number\") if num.text != None]\n",
        "            time.sleep(0.5)\n",
        "            \n",
        "            desText = [decs.text for decs in infoUList.find_all(\"h4\") if decs.text != None]\n",
        "            metaDataDict = {desText[string]: (dateList + liNumbers)[string] for string in range(len(desText))}\n",
        "            \n",
        "            if \"likes\" not in metaDataDict and \"like\" not in metaDataDict: metaDataDict[\"likes\"] = \"0\"\n",
        "            if \"links\" not in metaDataDict and \"link\" not in metaDataDict: metaDataDict[\"links\"]= \"0\"\n",
        "            \n",
        "            \n",
        "            \n",
        "            metaDataDict[\"date-scraped\"] = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
        "            print(metaDataDict)\n",
        "                \n",
        "            return metaDataDict\n",
        "    \n",
        "        except AttributeError as e:\n",
        "            \n",
        "            metaDataDict = {'date-scraped': f'{now.strftime(\"%d/%m/%Y %H:%M:%S\")}','created': f'{self.bSoup.find(\"span\", class_=\"relative-date\").get(\"title\")}', 'last reply': '0', 'replies': '0', 'views': 'Unknown', 'users': '1', 'likes': 'Unknown', 'links':'Unknown'}\n",
        "            print(metaDataDict)\n",
        "            \n",
        "            return metaDataDict\n",
        "            \n",
        "    \n",
        "    def getLink(self):\n",
        "        \n",
        "        self.blackList.add(\"/c/international\")\n",
        "\n",
        "        \n",
        "        for aTag in self.bSoup.find_all(\"a\"):\n",
        "            \n",
        "            try:\n",
        "                aTag = (aTag.get(\"href\"))\n",
        "                \n",
        "                \n",
        "\n",
        "                if aTag.startswith(\"/t/\") or aTag.startswith(\"/c/\") :\n",
        "                    self.linksStored.append(aTag)\n",
        "   \n",
        "                \n",
        "            except AttributeError as a:\n",
        "                continue\n",
        "        \n",
        "        self.linksStored = list(dict.fromkeys(self.linksStored))\n",
        "                \n",
        "    \n",
        "    def setLink(self, userRequest):\n",
        "        \n",
        "        \n",
        "\n",
        "        self.websitePage = random.choice([item for item in self.linksStored if isinstance(item, str) and userRequest in item])\n",
        "        self.currentLink = self.basicLink + self.websitePage\n",
        "        self.driver.get(self.currentLink)   \n",
        "        \n",
        "        self.linksStored.remove(self.websitePage)\n",
        "        self.blackList.add(self.websitePage)\n",
        "        \n",
        "        return self.currentLink, userRequest\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "JglHZNpfPGAZ"
      },
      "id": "JglHZNpfPGAZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "U2dUFtK2PGQt"
      },
      "id": "U2dUFtK2PGQt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "formNav = formNavigator(basicLink=\"https://forums.unrealengine.com\")\n",
        "\n",
        "startPageData = formNav.getPageHtml()\n",
        "wordScraper = discourseWordScraper(basicLink=startPageData[1], websitePage=startPageData[2], specificClasses=[\"cooked\"] )\n",
        "\n",
        "\n",
        "numIter = 0\n",
        "\n",
        "while numIter != 4:\n",
        "    \n",
        "    \n",
        "    if numIter >= 2: \n",
        "        \n",
        "        formNav.setLink(\"/t/\")\n",
        "        \n",
        "        formNav.scrollPage()\n",
        "        data = formNav.getPageHtml()\n",
        "        wordScraper.setVar(htmlData=str(data[0]), websitePage = data[2] )\n",
        "        html = wordScraper.getByClass()\n",
        "        wordScraper.saveCSV(finalInput=wordScraper.validateInput(tempData=html[0], typeCases=html[1], statsDict=data[3], repeating=True))\n",
        "        \n",
        "    \n",
        "    else: \n",
        "        formNav.getLink()\n",
        "        formNav.setLink(\"/c/\")\n",
        "        formNav.getPageHtml()\n",
        "        \n",
        "    numIter+=1"
      ],
      "metadata": {
        "id": "2xMhyISgMwpP"
      },
      "id": "2xMhyISgMwpP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "d7ecbeba",
      "metadata": {
        "scrolled": false,
        "id": "d7ecbeba"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16398de3",
      "metadata": {
        "id": "16398de3"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "name": "discourseScrapper-checkpoint.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}